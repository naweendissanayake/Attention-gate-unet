{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NyaTUrM7XbHk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Optional, Callable\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import os\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class CropSegmentationDataset(Dataset):\n",
    "    ROOT_PATH: str = \"/net/ens/am4ip/datasets/project-dataset/\"\n",
    "    id2cls: dict = {0: \"background\",\n",
    "                    1: \"crop\",\n",
    "                    2: \"weed\",\n",
    "                    3: \"partial-crop\",\n",
    "                    4: \"partial-weed\"}\n",
    "\n",
    "    cls2id: dict = {\"background\": 0,\n",
    "                    \"crop\": 1,\n",
    "                    \"weed\": 2,\n",
    "                    \"partial-crop\": 3,\n",
    "                    \"partial-weed\": 4}\n",
    "\n",
    "    def __init__(self, set_type: str = \"train\", transform: Optional[Callable] = None,\n",
    "                 target_transform: Optional[Callable] = None,\n",
    "                 merge_small_items: bool = True,\n",
    "                 remove_small_items: bool = False):\n",
    "        \"\"\"Class to load datasets for the Project.\n",
    "\n",
    "        Remark: `target_transform` is applied before merging items (this eases data augmentation).\n",
    "\n",
    "        :param set_type: Define if you load training, validation or testing sets. Should be either \"train\", \"val\" or \"test\".\n",
    "        :param transform: Callable to be applied on inputs.\n",
    "        :param target_transform: Callable to be applied on labels.\n",
    "        :param merge_small_items: Boolean to either merge classes of small or occluded objects.\n",
    "        :param remove_small_items: Boolean to consider as background class small or occluded objects. If `merge_small_items` is set to `True`, then this parameter is ignored.\n",
    "        \"\"\"\n",
    "        super(CropSegmentationDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.merge_small_items = merge_small_items\n",
    "        self.remove_small_items = remove_small_items\n",
    "\n",
    "        if set_type not in [\"train\", \"val\", \"test\"]:\n",
    "            raise ValueError(\"'set_type has an unknown value. \"\n",
    "                             f\"Got '{set_type}' but expected something in ['train', 'val', 'test'].\")\n",
    "\n",
    "        self.set_type = set_type\n",
    "        images = glob(os.path.join(self.ROOT_PATH, set_type, \"images/*\"))\n",
    "        images.sort()\n",
    "        self.images = np.array(images)\n",
    "\n",
    "        labels = glob(os.path.join(self.ROOT_PATH, set_type, \"labels/*\"))\n",
    "        labels.sort()\n",
    "        self.labels = np.array(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        input_img = Image.open(self.images[index], \"r\")\n",
    "        target = Image.open(self.labels[index], \"r\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            input_img = self.transform(input_img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        target_np = np.array(target)\n",
    "\n",
    "        if self.merge_small_items:\n",
    "            target_np[target_np == self.cls2id[\"partial-crop\"]] = self.cls2id[\"crop\"]\n",
    "            target_np[target_np == self.cls2id[\"partial-weed\"]] = self.cls2id[\"weed\"]\n",
    "        elif self.remove_small_items:\n",
    "            target_np[target_np == self.cls2id[\"partial-crop\"]] = self.cls2id[\"background\"]\n",
    "            target_np[target_np == self.cls2id[\"partial-weed\"]] = self.cls2id[\"background\"]\n",
    "\n",
    "        # Convert back to PIL image\n",
    "        target = Image.fromarray(target_np)\n",
    "\n",
    "        return input_img, target\n",
    "\n",
    "    def get_class_number(self):\n",
    "        if self.merge_small_items or self.remove_small_items:\n",
    "            return 3\n",
    "        else:\n",
    "            return 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "krnTmpnUhW6A"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(path):\n",
    "\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    img = img.astype('float32')\n",
    "    mx = np.max(img)\n",
    "    if mx:\n",
    "        img/=mx\n",
    "\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img_ten = torch.tensor(img)\n",
    "    return img_ten\n",
    "\n",
    "def preprocess_mask(path):\n",
    "\n",
    "    msk = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    msk = msk.astype('float32')\n",
    "    msk/=255.0\n",
    "    msk = np.repeat(msk[:, :, np.newaxis], 3, axis=-1)\n",
    "    msk_ten = torch.tensor(msk)\n",
    "    msk_ten= np.transpose(msk_ten, (2, 0, 1))\n",
    "\n",
    "    return msk_ten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "du8WxzfrGSd3"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_files, mask_files, input_size=(256, 256), augmentation_transforms=None):\n",
    "        self.image_files = image_files\n",
    "        self.mask_files = mask_files\n",
    "        self.input_size = input_size\n",
    "        self.augmentation_transforms = augmentation_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image_path = self.image_files[idx]\n",
    "        mask_path = self.mask_files[idx]\n",
    "\n",
    "        image = preprocess_image(image_path)\n",
    "        mask = preprocess_mask(mask_path)\n",
    "\n",
    "        if self.augmentation_transforms:\n",
    "            image, mask = self.augmentation_transforms(image, mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NN0o8CZ5hW_i"
   },
   "outputs": [],
   "source": [
    "def augment_image(image, mask):\n",
    "\n",
    "    image_np = image.permute(1, 2, 0).numpy()\n",
    "    mask_np = mask.permute(1, 2, 0).numpy()\n",
    "\n",
    "    transform = A.Compose([\n",
    "        A.Resize(256,256, interpolation=cv2.INTER_NEAREST),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "        A.RandomCrop(height=256, width=256, always_apply=True),\n",
    "        A.RandomBrightness(p=1),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.Blur(blur_limit=3, p=1),\n",
    "                A.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "    ])\n",
    "\n",
    "    augmented = transform(image=image_np, mask=mask_np)\n",
    "    augmented_image, augmented_mask = augmented['image'], augmented['mask']\n",
    "\n",
    "    augmented_image = torch.tensor(augmented_image, dtype=torch.float32).permute(2, 0, 1)\n",
    "    augmented_mask = torch.tensor(augmented_mask, dtype=torch.float32).permute(2, 0, 1)\n",
    "\n",
    "    return augmented_image, augmented_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBGKn0bKkq6y"
   },
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "qTh9ymuThXCX"
   },
   "outputs": [],
   "source": [
    "base_path = '/net/ens/am4ip/datasets/project-dataset/'\n",
    "\n",
    "dataset_train = '/net/ens/am4ip/datasets/project-dataset/train'\n",
    "dataset_val = '/net/ens/am4ip/datasets/project-dataset/val'\n",
    "\n",
    "\n",
    "images_path_train = os.path.join(base_path, dataset_train, 'images')\n",
    "labels_path_train = os.path.join(base_path, dataset_train, 'labels')\n",
    "\n",
    "images_path_val = os.path.join(base_path, dataset_val, 'images')\n",
    "labels_path_val = os.path.join(base_path, dataset_val, 'labels')\n",
    "\n",
    "images_train = sorted([os.path.join(images_path_train, f) for f in os.listdir(images_path_train) if f.endswith('.png')])\n",
    "label_train = sorted([os.path.join(labels_path_train, f) for f in os.listdir(labels_path_train) if f.endswith('.png')])\n",
    "\n",
    "images_val = sorted([os.path.join(images_path_val, f) for f in os.listdir(images_path_val) if f.endswith('.png')])\n",
    "label_val = sorted([os.path.join(labels_path_val, f) for f in os.listdir(labels_path_val) if f.endswith('.png')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4SfFGRbQhXEq"
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(images_train, label_train, augmentation_transforms=augment_image)\n",
    "val_dataset = CustomDataset(images_val, label_val, augmentation_transforms=augment_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fv5O_vtkixGc"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYCtpORjydlQ",
    "outputId": "8049ede0-5354-42f4-b59c-b4eabc20ed75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1407\n",
      "Validation dataset size: 422\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already created train_dataloader and val_dataloader\n",
    "train_dataset_size = len(train_dataloader.dataset)\n",
    "val_dataset_size = len(val_dataloader.dataset)\n",
    "\n",
    "print(\"Train dataset size:\", train_dataset_size)\n",
    "print(\"Validation dataset size:\", val_dataset_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "o7scNHrkixI1",
    "outputId": "030f0acb-7e6c-491d-a8af-90fff391d5cf"
   },
   "outputs": [],
   "source": [
    "# for batch_idx, (batch_images, batch_masks) in enumerate(train_dataloader):\n",
    "#     print(\"Batch\", batch_idx + 1)\n",
    "#     print(\"Image batch shape:\", batch_images.shape)\n",
    "#     print(\"Mask batch shape:\", batch_masks.shape)\n",
    "    \n",
    "#     for image, mask, image_path, mask_path in zip(batch_images, batch_masks, images_train, label_train):\n",
    "       \n",
    "#         image = image.permute((1, 2, 0)).numpy()*255.0\n",
    "#         image = image.astype('uint8')\n",
    "#         mask = (mask*255).numpy().astype('uint8')\n",
    "        \n",
    "#         image_filename = os.path.basename(image_path)\n",
    "#         mask_filename = os.path.basename(mask_path)\n",
    "        \n",
    "#         plt.figure(figsize=(15, 10))\n",
    "        \n",
    "#         plt.subplot(2, 4, 1)\n",
    "#         plt.imshow(image, cmap='gray')\n",
    "#         plt.title(f\"Original Image - {image_filename}\")\n",
    "        \n",
    "#         plt.subplot(2, 4, 2)\n",
    "#         plt.imshow(mask, cmap='gray')\n",
    "#         plt.title(f\"Mask Image - {mask_filename}\")\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v770AoiXnX4r",
    "outputId": "1d41c38a-e5f6-4927-e617-d6a270393ed6"
   },
   "outputs": [],
   "source": [
    "# for batch_idx, (batch_images, batch_masks) in enumerate(train_dataloader):\n",
    "#     print(\"Batch\", batch_idx + 1)\n",
    "#     print(\"Image batch shape:\", batch_images.shape)\n",
    "#     print(\"Mask batch shape:\", batch_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbXitC_NnZAZ",
    "outputId": "d0801981-4a01-4371-e7e7-143e5bad7e6f"
   },
   "outputs": [],
   "source": [
    "# for batch_idx, (batch_images, batch_masks) in enumerate(val_dataloader):\n",
    "#     print(\"Batch\", batch_idx + 1)\n",
    "#     print(\"Image batch shape:\", batch_images.shape)\n",
    "#     print(\"Mask batch shape:\", batch_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "7CIVTcSOixRY"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpConv, self).__init__()\n",
    "\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Attention block with learnable parameters\"\"\"\n",
    "\n",
    "    def __init__(self, F_g, F_l, n_coefficients):\n",
    "        \"\"\"\n",
    "        :param F_g: number of feature maps (channels) in previous layer\n",
    "        :param F_l: number of feature maps in corresponding encoder layer, transferred via skip connection\n",
    "        :param n_coefficients: number of learnable multi-dimensional attention coefficients\n",
    "        \"\"\"\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.W_gate = nn.Sequential(\n",
    "            nn.Conv2d(F_g, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(n_coefficients)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(n_coefficients)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(n_coefficients, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, gate, skip_connection):\n",
    "        \"\"\"\n",
    "        :param gate: gating signal from previous layer\n",
    "        :param skip_connection: activation from corresponding encoder layer\n",
    "        :return: output activations\n",
    "        \"\"\"\n",
    "        g1 = self.W_gate(gate)\n",
    "        x1 = self.W_x(skip_connection)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        psi = F.softmax(psi, dim=1)  # Apply softmax here\n",
    "\n",
    "        out = skip_connection * psi\n",
    "        return out\n",
    "\n",
    "class AttentionUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, img_ch=3, output_ch=3):\n",
    "        super(AttentionUNet, self).__init__()\n",
    "\n",
    "        self.MaxPool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = ConvBlock(img_ch, 64)\n",
    "        self.Conv2 = ConvBlock(64, 128)\n",
    "        self.Conv3 = ConvBlock(128, 256)\n",
    "        self.Conv4 = ConvBlock(256, 512)\n",
    "        self.Conv5 = ConvBlock(512, 1024)\n",
    "\n",
    "        self.Up5 = UpConv(1024, 512)\n",
    "        self.Att5 = AttentionBlock(F_g=512, F_l=512, n_coefficients=256)\n",
    "        self.UpConv5 = ConvBlock(1024, 512)\n",
    "\n",
    "        self.Up4 = UpConv(512, 256)\n",
    "        self.Att4 = AttentionBlock(F_g=256, F_l=256, n_coefficients=128)\n",
    "        self.UpConv4 = ConvBlock(512, 256)\n",
    "\n",
    "        self.Up3 = UpConv(256, 128)\n",
    "        self.Att3 = AttentionBlock(F_g=128, F_l=128, n_coefficients=64)\n",
    "        self.UpConv3 = ConvBlock(256, 128)\n",
    "\n",
    "        self.Up2 = UpConv(128, 64)\n",
    "        self.Att2 = AttentionBlock(F_g=64, F_l=64, n_coefficients=32)\n",
    "        self.UpConv2 = ConvBlock(128, 64)\n",
    "\n",
    "        self.Conv = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        e : encoder layers\n",
    "        d : decoder layers\n",
    "        s : skip-connections from encoder layers to decoder layers\n",
    "        \"\"\"\n",
    "        e1 = self.Conv1(x)\n",
    "\n",
    "        e2 = self.MaxPool(e1)\n",
    "        e2 = self.Conv2(e2)\n",
    "\n",
    "        e3 = self.MaxPool(e2)\n",
    "        e3 = self.Conv3(e3)\n",
    "\n",
    "        e4 = self.MaxPool(e3)\n",
    "        e4 = self.Conv4(e4)\n",
    "\n",
    "        e5 = self.MaxPool(e4)\n",
    "        e5 = self.Conv5(e5)\n",
    "\n",
    "        d5 = self.Up5(e5)\n",
    "\n",
    "        s4 = self.Att5(gate=d5, skip_connection=e4)\n",
    "        d5 = torch.cat((s4, d5), dim=1)\n",
    "        d5 = self.UpConv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        s3 = self.Att4(gate=d4, skip_connection=e3)\n",
    "        d4 = torch.cat((s3, d4), dim=1)\n",
    "        d4 = self.UpConv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        s2 = self.Att3(gate=d3, skip_connection=e2)\n",
    "        d3 = torch.cat((s2, d3), dim=1)\n",
    "        d3 = self.UpConv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        s1 = self.Att2(gate=d2, skip_connection=e1)\n",
    "        d2 = torch.cat((s1, d2), dim=1)\n",
    "        d2 = self.UpConv2(d2)\n",
    "\n",
    "        out = self.Conv(d2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dice_coeff(prediction, target):\n",
    "    # Move tensors to CPU\n",
    "    prediction_cpu = prediction.cpu().numpy()\n",
    "    target_cpu = target.cpu().numpy()\n",
    "\n",
    "    # Assuming that prediction and target have the same number of channels\n",
    "    num_channels = prediction_cpu.shape[1]\n",
    "\n",
    "    dice_scores = []\n",
    "\n",
    "    for channel in range(num_channels):\n",
    "        mask = np.zeros_like(prediction_cpu[:, channel, :, :])\n",
    "        mask[prediction_cpu[:, channel, :, :] >= 0.5] = 1\n",
    "\n",
    "        inter = np.sum(mask * target_cpu[:, channel, :, :])\n",
    "        union = np.sum(mask) + np.sum(target_cpu[:, channel, :, :])\n",
    "\n",
    "        dice = (2 * inter) / (union + 1e-8)  # Add a small epsilon to avoid division by zero\n",
    "        dice_scores.append(dice)\n",
    "\n",
    "    # Return the average dice score across channels\n",
    "    return np.mean(dice_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dice_coeff(prediction, target):\n",
    "    # Move tensors to CPU\n",
    "    prediction_cpu = prediction.cpu().numpy()\n",
    "    target_cpu = target.cpu().numpy()\n",
    "\n",
    "    # Assuming that prediction and target have the same number of channels\n",
    "    num_channels = prediction_cpu.shape[1]\n",
    "\n",
    "    dice_scores = []\n",
    "\n",
    "    if len(prediction_cpu.shape) == 4:\n",
    "        # Iterate over channels\n",
    "        for channel in range(num_channels):\n",
    "            mask = np.zeros_like(prediction_cpu[:, channel, :, :])\n",
    "            mask[prediction_cpu[:, channel, :, :] >= 0.5] = 1\n",
    "            inter = np.sum(mask * target_cpu[:, channel, :, :])\n",
    "            union = np.sum(mask) + np.sum(target_cpu[:, channel, :, :])\n",
    "            dice = (2 * inter) / (union + 1e-8)  # Add a small epsilon to avoid division by zero\n",
    "            dice_scores.append(dice)\n",
    "    elif len(prediction_cpu.shape) == 3:\n",
    "        # For binary segmentation\n",
    "        mask = np.zeros_like(prediction_cpu[:, 0, :, :])\n",
    "        mask[prediction_cpu[:, 0, :, :] >= 0.5] = 1\n",
    "        inter = np.sum(mask * target_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, size_average=None, ignore_index=-100, reduce=None, balance_param=1.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.size_average = size_average\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduce = reduce\n",
    "        self.balance_param = balance_param\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        print(input.shape, target.shape)\n",
    "\n",
    "        assert len(input.shape) == len(target.shape)\n",
    "\n",
    "        # Calculate probabilities and log probabilities\n",
    "        probs = F.softmax(input, dim=1)\n",
    "        log_probs = F.log_softmax(input, dim=1)\n",
    "\n",
    "        # Calculate focal loss\n",
    "        pt = torch.exp(log_probs) * (1 - probs)  # Fixing the one-hot encoding here\n",
    "        focal_loss = -((1 - pt) ** self.gamma) * log_probs\n",
    "\n",
    "        # Apply the ignore index if specified\n",
    "        if self.ignore_index >= 0:\n",
    "            ignore_mask = target != self.ignore_index\n",
    "            focal_loss = focal_loss[ignore_mask]\n",
    "            target = target[ignore_mask]\n",
    "\n",
    "        # Calculate the balanced focal loss\n",
    "        balanced_focal_loss = self.balance_param * focal_loss\n",
    "\n",
    "        # Take the mean over non-ignored samples\n",
    "        if self.size_average:\n",
    "            return balanced_focal_loss.mean()\n",
    "        elif self.reduce == 'sum':\n",
    "            return balanced_focal_loss.sum()\n",
    "        else:\n",
    "            return balanced_focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "IMrUZW1ti3iE"
   },
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'training': train_dataloader,\n",
    "    'test': val_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_and_test(model, dataloaders, optimizer, criterion, num_epochs=100, show_images=False):\n",
    "    since = time.time()\n",
    "    best_loss = float('inf')  # Initialize with a large value\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    fieldnames = ['epoch', 'training_loss', 'test_loss', 'training_dice_coeff', 'test_dice_coeff']\n",
    "    train_epoch_losses = []\n",
    "    test_epoch_losses = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        batchsummary = {a: [0] for a in fieldnames}\n",
    "        batch_train_loss = 0.0\n",
    "        batch_test_loss = 0.0\n",
    "\n",
    "        for phase in ['training', 'test']:\n",
    "            if phase == 'training':\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval() \n",
    "\n",
    "            for sample in iter(dataloaders[phase]):\n",
    "                inputs = sample[0].to(device)\n",
    "                masks = sample[1].to(device)\n",
    "                \n",
    "                #masks = masks.unsqueeze(1)\n",
    "                print('pipline mask',masks.shape)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'training'):\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    loss = criterion(outputs, masks)\n",
    "\n",
    "                    # Calculate multi-class Dice coefficient directly on the GPU\n",
    "                    y_pred = torch.argmax(outputs, dim=1)\n",
    "                    dice_coefficient = dice_coeff(y_pred, masks)\n",
    "\n",
    "                    batchsummary[f'{phase}_dice_coeff'].append(dice_coefficient.item())  # Convert to Python scalar\n",
    "\n",
    "                    if phase == 'training':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        batch_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                    else:\n",
    "                        batch_test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if phase == 'training':\n",
    "                epoch_train_loss = batch_train_loss / len(dataloaders['training'])\n",
    "                train_epoch_losses.append(epoch_train_loss)\n",
    "            else:\n",
    "                epoch_test_loss = batch_test_loss / len(dataloaders['test'])\n",
    "                test_epoch_losses.append(epoch_test_loss)\n",
    "\n",
    "            batchsummary['epoch'] = epoch\n",
    "            \n",
    "            print('{} Loss: {:.4f}'.format(phase, loss))\n",
    "\n",
    "        best_loss = np.min(batchsummary['test_dice_coeff'])\n",
    "        for field in fieldnames[3:]:\n",
    "            batchsummary[field] = np.mean(batchsummary[field])\n",
    "        print(\n",
    "            f'\\t\\t\\t train_dice_coeff: {batchsummary[\"training_dice_coeff\"]}, test_dice_coeff: {batchsummary[\"test_dice_coeff\"]}')\n",
    "\n",
    "    print('Best dice coefficient: {:4f}'.format(best_loss))\n",
    "\n",
    "    return model, train_epoch_losses, test_epoch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "Ls7kxTcvi3ni",
    "outputId": "8090a9da-8980-43c8-f56a-b3db9cabd57f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cremi/nmudiyansela/.local/lib/python3.11/site-packages/albumentations/augmentations/transforms.py:1258: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipline mask torch.Size([8, 3, 256, 256])\n",
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 3, 256, 256])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 3-dimensional, but 4 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m     trained_model, train_epoch_losses, test_epoch_losses \u001b[38;5;241m=\u001b[39m train_and_test(model, dataloaders, optimizer, criterion, num_epochs\u001b[38;5;241m=\u001b[39mepochs)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trained_model, train_epoch_losses, test_epoch_losses\n\u001b[0;32m---> 13\u001b[0m trained_model, train_epoch_losses, test_epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [34], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m      6\u001b[0m criterion \u001b[38;5;241m=\u001b[39m FocalLoss(gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m trained_model, train_epoch_losses, test_epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_model, train_epoch_losses, test_epoch_losses\n",
      "Cell \u001b[0;32mIn [33], line 49\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[0;34m(model, dataloaders, optimizer, criterion, num_epochs, show_images)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Calculate multi-class Dice coefficient directly on the GPU\u001b[39;00m\n\u001b[1;32m     48\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m dice_coefficient \u001b[38;5;241m=\u001b[39m \u001b[43mdice_coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m batchsummary[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_dice_coeff\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(dice_coefficient\u001b[38;5;241m.\u001b[39mitem())  \u001b[38;5;66;03m# Convert to Python scalar\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn [30], line 24\u001b[0m, in \u001b[0;36mdice_coeff\u001b[0;34m(prediction, target)\u001b[0m\n\u001b[1;32m     21\u001b[0m         dice_scores\u001b[38;5;241m.\u001b[39mappend(dice)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prediction_cpu\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# For binary segmentation\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[43mprediction_cpu\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     25\u001b[0m     mask[prediction_cpu[:, \u001b[38;5;241m0\u001b[39m, :, :] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     26\u001b[0m     inter \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(mask \u001b[38;5;241m*\u001b[39m target_cpu)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 3-dimensional, but 4 were indexed"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "\n",
    "def train():\n",
    "    model = AttentionUNet()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = FocalLoss(gamma=2)\n",
    "\n",
    "    trained_model, train_epoch_losses, test_epoch_losses = train_and_test(model, dataloaders, optimizer, criterion, num_epochs=epochs)\n",
    "\n",
    "    return trained_model, train_epoch_losses, test_epoch_losses\n",
    "\n",
    "\n",
    "trained_model, train_epoch_losses, test_epoch_losses = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hc1Jx0SRi3qE"
   },
   "outputs": [],
   "source": [
    "pth_weigth_file = torch.save(trained_model.state_dict(), 'trained_model.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
