{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyaTUrM7XbHk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Optional, Callable\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import os\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class CropSegmentationDataset(Dataset):\n",
    "    ROOT_PATH: str = \"/net/ens/am4ip/datasets/project-dataset/\"\n",
    "    id2cls: dict = {0: \"background\",\n",
    "                    1: \"crop\",\n",
    "                    2: \"weed\",\n",
    "                    3: \"partial-crop\",\n",
    "                    4: \"partial-weed\"}\n",
    "\n",
    "    cls2id: dict = {\"background\": 0,\n",
    "                    \"crop\": 1,\n",
    "                    \"weed\": 2,\n",
    "                    \"partial-crop\": 3,\n",
    "                    \"partial-weed\": 4}\n",
    "\n",
    "    def __init__(self, set_type: str = \"train\", transform: Optional[Callable] = None,\n",
    "                 target_transform: Optional[Callable] = None,\n",
    "                 merge_small_items: bool = True,\n",
    "                 remove_small_items: bool = False):\n",
    "        \"\"\"Class to load datasets for the Project.\n",
    "\n",
    "        Remark: `target_transform` is applied before merging items (this eases data augmentation).\n",
    "\n",
    "        :param set_type: Define if you load training, validation or testing sets. Should be either \"train\", \"val\" or \"test\".\n",
    "        :param transform: Callable to be applied on inputs.\n",
    "        :param target_transform: Callable to be applied on labels.\n",
    "        :param merge_small_items: Boolean to either merge classes of small or occluded objects.\n",
    "        :param remove_small_items: Boolean to consider as background class small or occluded objects. If `merge_small_items` is set to `True`, then this parameter is ignored.\n",
    "        \"\"\"\n",
    "        super(CropSegmentationDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.merge_small_items = merge_small_items\n",
    "        self.remove_small_items = remove_small_items\n",
    "\n",
    "        if set_type not in [\"train\", \"val\", \"test\"]:\n",
    "            raise ValueError(\"'set_type has an unknown value. \"\n",
    "                             f\"Got '{set_type}' but expected something in ['train', 'val', 'test'].\")\n",
    "\n",
    "        self.set_type = set_type\n",
    "        images = glob(os.path.join(self.ROOT_PATH, set_type, \"images/*\"))\n",
    "        images.sort()\n",
    "        self.images = np.array(images)\n",
    "\n",
    "        labels = glob(os.path.join(self.ROOT_PATH, set_type, \"labels/*\"))\n",
    "        labels.sort()\n",
    "        self.labels = np.array(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        input_img = Image.open(self.images[index], \"r\")\n",
    "        target = Image.open(self.labels[index], \"r\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            input_img = self.transform(input_img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        target_np = np.array(target)\n",
    "\n",
    "        if self.merge_small_items:\n",
    "            target_np[target_np == self.cls2id[\"partial-crop\"]] = self.cls2id[\"crop\"]\n",
    "            target_np[target_np == self.cls2id[\"partial-weed\"]] = self.cls2id[\"weed\"]\n",
    "        elif self.remove_small_items:\n",
    "            target_np[target_np == self.cls2id[\"partial-crop\"]] = self.cls2id[\"background\"]\n",
    "            target_np[target_np == self.cls2id[\"partial-weed\"]] = self.cls2id[\"background\"]\n",
    "\n",
    "        # Convert back to PIL image\n",
    "        target = Image.fromarray(target_np)\n",
    "\n",
    "        return input_img, target\n",
    "\n",
    "    def get_class_number(self):\n",
    "        if self.merge_small_items or self.remove_small_items:\n",
    "            return 3\n",
    "        else:\n",
    "            return 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krnTmpnUhW6A"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(path):\n",
    "\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    img = img.astype('float32')\n",
    "    mx = np.max(img)\n",
    "    if mx:\n",
    "        img/=mx\n",
    "\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img_ten = torch.tensor(img)\n",
    "    return img_ten\n",
    "\n",
    "def preprocess_mask(path):\n",
    "\n",
    "    msk = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    msk = msk.astype('float32')\n",
    "    msk/=255.0\n",
    "    msk = np.repeat(msk[:, :, np.newaxis], 3, axis=-1)\n",
    "    msk_ten = torch.tensor(msk)\n",
    "    msk_ten= np.transpose(msk_ten, (2, 0, 1))\n",
    "\n",
    "    return msk_ten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "du8WxzfrGSd3"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_files, mask_files, input_size=(256, 256), augmentation_transforms=None):\n",
    "        self.image_files = image_files\n",
    "        self.mask_files = mask_files\n",
    "        self.input_size = input_size\n",
    "        self.augmentation_transforms = augmentation_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image_path = self.image_files[idx]\n",
    "        mask_path = self.mask_files[idx]\n",
    "\n",
    "        image = preprocess_image(image_path)\n",
    "        mask = preprocess_mask(mask_path)\n",
    "\n",
    "        if self.augmentation_transforms:\n",
    "            image, mask = self.augmentation_transforms(image, mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NN0o8CZ5hW_i"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import albumentations as A\n",
    "\n",
    "def bilateral_filter(img, d=9, sigma_color=75, sigma_space=75):\n",
    "    return cv2.bilateralFilter(img, d, sigma_color, sigma_space)\n",
    "\n",
    "def augment_image(image, mask):\n",
    "    image_np = image.permute(1, 2, 0).numpy()\n",
    "    mask_np = mask.permute(1, 2, 0).numpy()\n",
    "\n",
    "    transform = A.Compose([\n",
    "        A.Resize(256, 256, interpolation=cv2.INTER_NEAREST),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "        A.RandomCrop(height=256, width=256, always_apply=True),\n",
    "        A.RandomBrightness(p=1),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.Blur(blur_limit=3, p=0.5),\n",
    "                A.MotionBlur(blur_limit=3, p=0.5),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    augmented = transform(image=image_np, mask=mask_np)\n",
    "    augmented_image, augmented_mask = augmented['image'], augmented['mask']\n",
    "\n",
    "    # Apply bilateral filter separately to the image and mask\n",
    "    augmented_image = bilateral_filter(augmented_image, d=9, sigma_color=75, sigma_space=75)\n",
    "    augmented_mask = bilateral_filter(augmented_mask, d=9, sigma_color=75, sigma_space=75)\n",
    "\n",
    "    augmented_image = torch.tensor(augmented_image, dtype=torch.float32).permute(2, 0, 1)\n",
    "    augmented_mask = torch.tensor(augmented_mask, dtype=torch.float32).permute(2, 0, 1)\n",
    "\n",
    "    return augmented_image, augmented_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBGKn0bKkq6y"
   },
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTh9ymuThXCX"
   },
   "outputs": [],
   "source": [
    "base_path = '/net/ens/am4ip/datasets/project-dataset/'\n",
    "\n",
    "dataset_train = '/net/ens/am4ip/datasets/project-dataset/train'\n",
    "dataset_val = '/net/ens/am4ip/datasets/project-dataset/val'\n",
    "\n",
    "\n",
    "images_path_train = os.path.join(base_path, dataset_train, 'images')\n",
    "labels_path_train = os.path.join(base_path, dataset_train, 'labels')\n",
    "\n",
    "images_path_val = os.path.join(base_path, dataset_val, 'images')\n",
    "labels_path_val = os.path.join(base_path, dataset_val, 'labels')\n",
    "\n",
    "images_train = sorted([os.path.join(images_path_train, f) for f in os.listdir(images_path_train) if f.endswith('.png')])\n",
    "label_train = sorted([os.path.join(labels_path_train, f) for f in os.listdir(labels_path_train) if f.endswith('.png')])\n",
    "\n",
    "images_val = sorted([os.path.join(images_path_val, f) for f in os.listdir(images_path_val) if f.endswith('.png')])\n",
    "label_val = sorted([os.path.join(labels_path_val, f) for f in os.listdir(labels_path_val) if f.endswith('.png')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SfFGRbQhXEq"
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(images_train, label_train, augmentation_transforms=augment_image)\n",
    "val_dataset = CustomDataset(images_val, label_val, augmentation_transforms=augment_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fv5O_vtkixGc"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYCtpORjydlQ",
    "outputId": "4c89c426-8cf5-4e61-e978-cae830e36228"
   },
   "outputs": [],
   "source": [
    "# Assuming you have already created train_dataloader and val_dataloader\n",
    "train_dataset_size = len(train_dataloader.dataset)\n",
    "val_dataset_size = len(val_dataloader.dataset)\n",
    "\n",
    "print(\"Train dataset size:\", train_dataset_size)\n",
    "print(\"Validation dataset size:\", val_dataset_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7scNHrkixI1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "for batch_idx, (batch_images, batch_masks) in enumerate(train_dataloader):\n",
    "    print(\"Batch\", batch_idx + 1)\n",
    "    print(\"Image batch shape:\", batch_images.shape)\n",
    "    print(\"Mask batch shape:\", batch_masks.shape)\n",
    "\n",
    "    for image, mask, image_path, mask_path in zip(batch_images, batch_masks, images_train, label_train):\n",
    "        image = image.permute((1, 2, 0)).numpy()\n",
    "        image = (image * 255.0).astype('uint8')\n",
    "\n",
    "        # Assuming a multichannel mask\n",
    "        mask = mask.permute((1, 2, 0)).numpy()\n",
    "        mask = (mask * 255.0).astype('uint8')\n",
    "\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        mask_filename = os.path.basename(mask_path)\n",
    "\n",
    "        plt.figure(figsize=(15, 10))\n",
    "\n",
    "        plt.subplot(2, 4, 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f\"Original Image - {image_filename}\")\n",
    "\n",
    "        # Visualize each channel of the mask separately\n",
    "        num_channels = mask.shape[2]\n",
    "        for channel in range(num_channels):\n",
    "            plt.subplot(2, 4, channel + 2)\n",
    "            plt.imshow(mask[:, :, channel], cmap='viridis')\n",
    "            plt.title(f\"Mask Channel {channel + 1}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v770AoiXnX4r"
   },
   "outputs": [],
   "source": [
    "for batch_idx, (batch_images, batch_masks) in enumerate(train_dataloader):\n",
    "    print(\"Batch\", batch_idx + 1)\n",
    "    print(\"Image batch shape:\", batch_images.shape)\n",
    "    print(\"Mask batch shape:\", batch_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbXitC_NnZAZ"
   },
   "outputs": [],
   "source": [
    "for batch_idx, (batch_images, batch_masks) in enumerate(val_dataloader):\n",
    "    print(\"Batch\", batch_idx + 1)\n",
    "    print(\"Image batch shape:\", batch_images.shape)\n",
    "    print(\"Mask batch shape:\", batch_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1PPVfUdiPgE",
    "outputId": "3373b91e-f44e-4212-ac66-aad1a7646f8b"
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CIVTcSOixRY"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpConv, self).__init__()\n",
    "\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Attention block with learnable parameters\"\"\"\n",
    "\n",
    "    def __init__(self, F_g, F_l, n_coefficients):\n",
    "        \"\"\"\n",
    "        :param F_g: number of feature maps (channels) in previous layer\n",
    "        :param F_l: number of feature maps in corresponding encoder layer, transferred via skip connection\n",
    "        :param n_coefficients: number of learnable multi-dimensional attention coefficients\n",
    "        \"\"\"\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.W_gate = nn.Sequential(\n",
    "            nn.Conv2d(F_g, n_coefficients, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(n_coefficients)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, n_coefficients, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(n_coefficients)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(n_coefficients, 1, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(1),\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, gate, skip_connection):\n",
    "        \"\"\"\n",
    "        :param gate: gating signal from previous layer\n",
    "        :param skip_connection: activation from corresponding encoder layer\n",
    "        :return: output activations\n",
    "        \"\"\"\n",
    "        g1 = self.W_gate(gate)\n",
    "        x1 = self.W_x(skip_connection)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        psi = F.softmax(psi, dim=1)  \n",
    "\n",
    "        out = skip_connection * psi\n",
    "        return out\n",
    "\n",
    "class AttentionUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, img_ch=3, output_ch=3):\n",
    "        super(AttentionUNet, self).__init__()\n",
    "\n",
    "        self.MaxPool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = ConvBlock(img_ch, 64)\n",
    "        self.Conv2 = ConvBlock(64, 128)\n",
    "        self.Conv3 = ConvBlock(128, 256)\n",
    "        self.Conv4 = ConvBlock(256, 512)\n",
    "        self.Conv5 = ConvBlock(512, 1024)\n",
    "\n",
    "        self.Up5 = UpConv(1024, 512)\n",
    "        self.Att5 = AttentionBlock(F_g=512, F_l=512, n_coefficients=256)\n",
    "        self.UpConv5 = ConvBlock(1024, 512)\n",
    "\n",
    "        self.Up4 = UpConv(512, 256)\n",
    "        self.Att4 = AttentionBlock(F_g=256, F_l=256, n_coefficients=128)\n",
    "        self.UpConv4 = ConvBlock(512, 256)\n",
    "\n",
    "        self.Up3 = UpConv(256, 128)\n",
    "        self.Att3 = AttentionBlock(F_g=128, F_l=128, n_coefficients=64)\n",
    "        self.UpConv3 = ConvBlock(256, 128)\n",
    "\n",
    "        self.Up2 = UpConv(128, 64)\n",
    "        self.Att2 = AttentionBlock(F_g=64, F_l=64, n_coefficients=32)\n",
    "        self.UpConv2 = ConvBlock(128, 64)\n",
    "\n",
    "        self.Conv = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        e : encoder layers\n",
    "        d : decoder layers\n",
    "        s : skip-connections from encoder layers to decoder layers\n",
    "        \"\"\"\n",
    "        e1 = self.Conv1(x)\n",
    "\n",
    "        e2 = self.MaxPool(e1)\n",
    "        e2 = self.Conv2(e2)\n",
    "\n",
    "        e3 = self.MaxPool(e2)\n",
    "        e3 = self.Conv3(e3)\n",
    "\n",
    "        e4 = self.MaxPool(e3)\n",
    "        e4 = self.Conv4(e4)\n",
    "\n",
    "        e5 = self.MaxPool(e4)\n",
    "        e5 = self.Conv5(e5)\n",
    "\n",
    "        d5 = self.Up5(e5)\n",
    "\n",
    "        s4 = self.Att5(gate=d5, skip_connection=e4)\n",
    "        d5 = torch.cat((s4, d5), dim=1)\n",
    "        d5 = self.UpConv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        s3 = self.Att4(gate=d4, skip_connection=e3)\n",
    "        d4 = torch.cat((s3, d4), dim=1)\n",
    "        d4 = self.UpConv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        s2 = self.Att3(gate=d3, skip_connection=e2)\n",
    "        d3 = torch.cat((s2, d3), dim=1)\n",
    "        d3 = self.UpConv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        s1 = self.Att2(gate=d2, skip_connection=e1)\n",
    "        d2 = torch.cat((s1, d2), dim=1)\n",
    "        d2 = self.UpConv2(d2)\n",
    "\n",
    "        out = self.Conv(d2)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGe8b7wJiPgF"
   },
   "outputs": [],
   "source": [
    "def dice_coeff(prediction, target, num_classes, epsilon=1e-4):\n",
    "    # Move tensors to CPU\n",
    "    prediction_cpu = prediction.cpu().numpy()\n",
    "    target_cpu = target.cpu().numpy()\n",
    "\n",
    "    dice_scores = []\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        # Compute dice for each class\n",
    "        intersection = np.sum((prediction_cpu == class_idx) & (target_cpu[:, class_idx, :, :] == class_idx))\n",
    "        union = np.sum((prediction_cpu == class_idx) | (target_cpu[:, class_idx, :, :] == class_idx))\n",
    "        dice = (2.0 * intersection) / (union + epsilon)\n",
    "        dice_scores.append(dice)\n",
    "\n",
    "    # Return the average dice score across classes\n",
    "    return np.mean(dice_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwxVfWgXiPgF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, size_average=None, ignore_index=-100, reduce=None, balance_param=1.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.size_average = size_average\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduce = reduce\n",
    "        self.balance_param = balance_param\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        #print(input.shape, target.shape)\n",
    "\n",
    "        assert len(input.shape) == len(target.shape)\n",
    "\n",
    "        # Calculate probabilities and log probabilities\n",
    "        probs = F.softmax(input, dim=1)\n",
    "        log_probs = F.log_softmax(input, dim=1)\n",
    "\n",
    "        # Calculate focal loss\n",
    "        pt = torch.exp(log_probs) * (1 - probs)  # Fixing the one-hot encoding here\n",
    "        focal_loss = -((1 - pt) ** self.gamma) * log_probs\n",
    "\n",
    "        # Apply the ignore index if specified\n",
    "        if self.ignore_index >= 0:\n",
    "            ignore_mask = target != self.ignore_index\n",
    "            focal_loss = focal_loss[ignore_mask]\n",
    "            target = target[ignore_mask]\n",
    "\n",
    "        # Calculate the balanced focal loss\n",
    "        balanced_focal_loss = self.balance_param * focal_loss\n",
    "\n",
    "        # Take the mean over non-ignored samples\n",
    "        if self.size_average:\n",
    "            return balanced_focal_loss.mean()\n",
    "        elif self.reduce == 'sum':\n",
    "            return balanced_focal_loss.sum()\n",
    "        else:\n",
    "            return balanced_focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMrUZW1ti3iE"
   },
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'training': train_dataloader,\n",
    "    'test': val_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pYs_dAgiPgG"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_and_test(model, dataloaders, optimizer, criterion, num_epochs=100, show_images=False):\n",
    "    since = time.time()\n",
    "    best_loss = float('inf')  # Initialize with a large value\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    fieldnames = ['epoch', 'training_loss', 'test_loss', 'training_dice_coeff', 'test_dice_coeff']\n",
    "    train_epoch_losses = []\n",
    "    test_epoch_losses = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "        print(f'Epoch {epoch}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        batchsummary = {a: [0] for a in fieldnames}\n",
    "        batch_train_loss = 0.0\n",
    "        batch_test_loss = 0.0\n",
    "\n",
    "        for phase in ['training', 'test']:\n",
    "            if phase == 'training':\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval() \n",
    "\n",
    "            for sample in iter(dataloaders[phase]):\n",
    "                inputs = sample[0].to(device)\n",
    "                masks = sample[1].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'training'):\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    loss = criterion(outputs, masks)\n",
    "                    loss = loss.mean()\n",
    "\n",
    "                    # Calculate multi-class Dice coefficient directly on the GPU\n",
    "                    y_pred = torch.argmax(outputs, dim=1)\n",
    "                    dice_coefficient = dice_coeff(y_pred, masks, 3)\n",
    "\n",
    "                    batchsummary[f'{phase}_dice_coeff'].append(dice_coefficient.item())  # Convert to Python scalar\n",
    "\n",
    "                    if phase == 'training':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        batch_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                    else:\n",
    "                        batch_test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if phase == 'training':\n",
    "                epoch_train_loss = batch_train_loss / len(dataloaders['training'])\n",
    "                train_epoch_losses.append(epoch_train_loss)\n",
    "            else:\n",
    "                epoch_test_loss = batch_test_loss / len(dataloaders['test'])\n",
    "                test_epoch_losses.append(epoch_test_loss)\n",
    "\n",
    "            batchsummary['epoch'] = epoch\n",
    "            \n",
    "            print('{} Loss: {:.4f}'.format(phase, loss))\n",
    "\n",
    "        best_loss = np.min(batchsummary['test_dice_coeff'])\n",
    "        for field in fieldnames[3:]:\n",
    "            batchsummary[field] = np.mean(batchsummary[field])\n",
    "        print(\n",
    "            f'\\t\\t\\t train_dice_coeff: {batchsummary[\"training_dice_coeff\"]}, test_dice_coeff: {batchsummary[\"test_dice_coeff\"]}')\n",
    "\n",
    "        # Visualize some results\n",
    "        if show_images and epoch % 5 == 0:\n",
    "            visualize_results(model, dataloaders['test'], device)\n",
    "\n",
    "    print('Best dice coefficient: {:4f}'.format(best_loss))\n",
    "\n",
    "    return model, train_epoch_losses, test_epoch_losses\n",
    "\n",
    "def visualize_results(model, dataloader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample in iter(dataloader):\n",
    "            inputs = sample[0].to(device)\n",
    "            masks = sample[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Convert tensors to NumPy arrays for visualization\n",
    "            inputs_np = inputs.cpu().numpy()\n",
    "            masks_np = masks.cpu().numpy()\n",
    "            predictions_np = predictions.cpu().numpy()\n",
    "\n",
    "            # Display images, ground truth masks, and predicted masks\n",
    "            for i in range(inputs_np.shape[0]):\n",
    "                plt.figure(figsize=(12, 4))\n",
    "\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(inputs_np[i].transpose(1, 2, 0))\n",
    "                plt.title('Input Image')\n",
    "\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.imshow(masks_np[i, 0, :, :], cmap='viridis')\n",
    "                plt.title('Ground Truth Mask')\n",
    "\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.imshow(predictions_np[i, :, :], cmap='viridis')\n",
    "                plt.title('Predicted Mask')\n",
    "\n",
    "                plt.show()\n",
    "                break  # Display only the first batch for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ls7kxTcvi3ni",
    "outputId": "eb18bc5b-0fc9-4a4f-a0ed-3784e57c1906"
   },
   "outputs": [],
   "source": [
    "epochs=50\n",
    "def train():\n",
    "    model = AttentionUNet()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = FocalLoss(gamma=1)\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    trained_model, train_epoch_losses, test_epoch_losses = train_and_test(model, dataloaders, \n",
    "                                            optimizer, criterion, num_epochs=epochs,show_images=True)\n",
    "\n",
    "    return trained_model, train_epoch_losses, test_epoch_losses\n",
    "\n",
    "\n",
    "trained_model, train_epoch_losses, test_epoch_losses = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "7e00pX3L3jMj",
    "outputId": "6679f967-5689-4051-abf8-dadc86a36ab2"
   },
   "outputs": [],
   "source": [
    "train_plot, = plt.plot(range(1, len(train_epoch_losses) + 1), train_epoch_losses, label='train loss')\n",
    "test_plot, = plt.plot(range(1, len(test_epoch_losses) + 1), test_epoch_losses, label='test loss')\n",
    "plt.legend(handles=[train_plot, test_plot])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "m1AZtbv43mjm",
    "outputId": "5900a44c-dae0-4473-a24c-18181b78ab12"
   },
   "outputs": [],
   "source": [
    "train_plot, = plt.plot(range(len(train_epoch_losses)-15), train_epoch_losses[15:], label='train loss')\n",
    "test_plot, = plt.plot(range(len(test_epoch_losses)-15), test_epoch_losses[15:], label='test loss')\n",
    "plt.legend(handles=[train_plot, test_plot])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hc1Jx0SRi3qE"
   },
   "outputs": [],
   "source": [
    "pth_weigth_file = torch.save(trained_model.state_dict(), 'trained_model.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
